#### 查询

# **一.背景**

## **1.1 项目背景**

### newton 2.0项目适配、写入、查询sdk包都融合在一个服务中，耦合较重，没有进行拆分。特征3.0重构时把适配、写入、查询服务独立，解耦。

## **1.2 项目定位**

### 特征写入服务，包括特征抽取、转换、存储。

# **二.系统总体设计**

## **2.1 系统框架图**



监控告警：



## **2.2 系统代码层级模块**

**

##### 父模块：

##### uniform-feature-core

##### 子模块：

##### uniform-feature-core-api ：负责提供外部rpc接口模块； uniform-feature-core-biz/uniform-feature-core-domain ：负责存放核心基础类模块； uniform-feature-core-biz/uniform-feature-core-dal ：负责与数据库交互功能模块，如：mysql、redis等； uniform-feature-core-biz/uniform-feature-core-integration ： 负责调用外部依赖服务模块； uniform-feature-core-biz/uniform-feature-core-service ：负责服务功能实现模块； uniform-feature-core-server ：服务直接与上层交互模块，如:kafka/RocketMQ、rest接口等；

# **三.功能模块**

### 3.1.2 处理流程

业务处理框图



强实时特征写入流程



### 关联db模型



数据处理流程图：



 

数据抽取部分缓存如下所示，其他模块雷同





### admin与core服务交互

特征源发布

![img]()



特征集发布



特征集停用



特征源推数



特征集推数







**特征数据准备服务,包含但不限于货源基本数据（货起始地、目的地地，车长等），车辆规格数据（车长等），用户浏览货源、交易货源的数量，线路货源数量，线路货源平均价格等各种数据指标的收集、归类、预处理等操作。**







写入服务历史峰值50wqps，目前峰值25wqps, 同时写入四个redis集群，其中两个双写

核心业务场景的写入保障：

双写

实时特征topic隔离

强实时特征单独链路



kafka消息堆积问题：

离线实时特征topic隔离，确保实时特征及时消费



读取服务峰值10wqps, 调用方42个



mget rt比单条查询高，存在大量超时

优化：采用近端缓存，减少io次数， 超时时间减少到50ms，超时重试



**特征写入kafka积压原因，瓶颈点详细分析，如线程、容器、redis等**

1.线程池满，由调度线程串行执行，处理效率低导致积压

2.容器cpu使用率已达瓶颈

3.redis集群性能已达瓶颈

4.业务代码效率低下，处理能力慢导致积压

5.partiton分配不均，如partiton数量少于容器数量，分配partiton的容器处理不过来导致积压

6.partiton分配不均，个别容器分配partiton较多导致积压



冷热分离场景目前主要针对司机，货主，路线等维度的离线特征；例如，司机全量有2千万+，但是日活只有3百万左右； 这些数据如果全部放到redis集群，那么只有不到20%的数据才会被使用，存在大量占用redis内存，写入tps和带宽等资源的问题； 因此，我们采用离线的近N日活跃司机作为热点司机的基数，以司机ID作为主键的离线特征集写入特征时，判断是否热点司机，若是热点，则写入redis;不是则写入HBase. 
当日新用户登录时，监听MQ消息，判断是否为热点司机，若是，则查redis;  若不是，则查HBase, 并发一条加载该司机特征的消息，下游监听消息，并主动写入redis,并更新热点司机列表。



特征存储key   特征源名:源ID:主键